---
title: "社内向け勤怠・人事情報管理システム運用保守PJ"
---

## 社内向けPHPサイト / Ubuntuサーバ運用保守

### 概要

#### ドメイン

人事・総務系の社内業務システム（勤怠・経費・資格情報・ヘルスケア等）を対象とした社員向けポータル。

#### 目的

- 前担当者からの運用保守業務の引継ぎと、安定運用の継続
- 人事異動に伴うメール通知先・各種管理権限の正確な更新
- PHP・OS・ライブラリ等のバージョン陳腐化に対する計画的なバージョンアップ
- 障害・バグ発生時の原因調査と恒久対応による運用リスクの低減

#### 構成

PHP（Smarty, phpSpreadsheet）＋ Apache2 ＋ Ubuntu サーバ  
＋ PostgreSQL ＋ CSV ファイル連携 ＋ シェルスクリプト ＋ cron ジョブ ＋ Samba 共有

### 案件情報

期間：2025/02 〜 現在  

体制：PM 1名、SE 1〜3名からなる 2〜4名の受託開発チーム。  

役割：PHP / シェルの改修・調査、日次運用、障害対応など運用保守全般を担当。

### 担当業務

- 勤怠・経費関連 CSV の日次更新確認（毎朝 / 業務開始前）
- 更新用データの不整合調査と手動補正（CSV / DB）
- 社員向けポータルの管理者権限の追加・削除（人事異動・組織改編対応）
- サービスダウン・画面エラー発生時の一次切り分け・復旧対応
- 既存 PHP / シェル実装の機能調査と仕様整理（ブラックボックス化した処理の可視化）
- PHP・シェルスクリプトの臨時実行（突発的なデータ再処理・リカバリ対応）
- メール送信機能の文面・宛先ロジックの改修（人事・総務部門からの要望反映）
- WSLを使用した検証環境の構築・運用(本番環境で発生した不具合の原因究明目的)

### 技術スタック

**言語・ライブラリ:** PHP, シェルスクリプト, Smarty, phpSpreadsheet  

**DB:** PostgreSQL  

**OS・ミドルウェア:** Ubuntu, Apache2, Samba, cron  

**ツール:** VSCode, WSL, Tera Term, サクラエディタ, phpPgAdmin, WinSCP, composer

### Key ADR

#### ADR-01: PostgreSQLサービスダウン（ディスクフル）

##### 背景

バックアップ用の `tar.gz` フォルダを大量に生成していたことが原因で、サーバのディスク空き容量が急激に低下し、PostgreSQLサービスが頻繁にダウンする事象が発生した。  
ルートパーティション配下の容量圧迫状況を調査した結果、古いバックアップファイル群が想定以上に蓄積されており、PostgreSQLのデータディレクトリを含むファイルシステムの空き容量を圧迫していた。

##### 評価軸

| 評価軸 | 案A：ディスク容量の恒久対策<br>（ディスク拡張／バックアップ保存先移行） | 案B：バックアップファイル削除による即時復旧 | 案C：PostgreSQL再起動・設定変更による暫定対応 |
|---|---|---|---|
| 即時復旧性 | △ 大容量ディスク追加やパーティション変更が必要で、復旧までに時間を要する | ◎ 不要世代の削除のみで短時間に空き容量を確保可能 | △ 再起動で一時的に復旧しても、ディスクフルが解消されず再発リスクが高い |
| 影響範囲の限定しやすさ | △ パーティション構成やマウントポイントの変更が入り、影響範囲が広くなりがち | 〇 削除対象のバックアップ世代を明確に絞れば、影響範囲を限定しやすい | △ サービス全体に対する再起動・設定変更となり、副作用の切り分けが難しい |
| 作業工数・調整コスト | ✕ 設計・申請・メンテ・検証が必要で、運用部門との調整コストも大きい | 〇 手順がシンプルで、少人数・短時間で実施可能 | 〇 コマンド自体は軽いが、原因未解消のまま再発対応を繰り返す工数が発生する |
| 長期的な再発防止性 | 〇 容量に余裕ができるため一定の効果はあるが、バックアップ肥大の根本原因は別途管理が必要 | △ 削除後も運用ルールを整えないと、将来的に再度逼迫する可能性がある | ✕ ディスク使用量は変わらず、同じ条件で再びディスクフルに至る可能性が高い |
| 業務・監査への影響 | 〇 バックアップ世代数を維持したまま容量だけ増やせるため、監査影響は小さい    | △ 削除対象が監査要件を満たすか事前確認が必要 | 〇 データ削除は行わず、業務影響は直接的には小さいが、再発時に業務停止リスクが残る |

##### 採否

- **案A：ディスク容量の恒久対策**  
  → **却下**

- **案B：バックアップファイル削除による即時復旧**  
  → **採用**

- **案C：PostgreSQL再起動・設定変更による暫定対応**  
  → **却下**

##### 却下理由

- **案A：ディスク容量の恒久対策（ディスク拡張／バックアップ保存先移行）**  
  ディスク構成の変更や新規ディスク追加には、設計・申請・検証・作業時間が必要になり、障害対応として求められる速やかな復旧には間に合わないおそれがある。  
  この時点で容量を圧迫している主因がバックアップ用の `tar.gz` ファイル群であることは特定できており、それらを削除すれば復旧できる見通しが立っていた。  
  そのため、案Aは本インシデントの初動対応ではなく、後続の恒久対策候補と位置づけて採用しないことにした。

- **案C：PostgreSQL再起動・設定変更による暫定対応**  
  `systemctl` による再起動や設定変更だけではディスクフルという根本原因を取り除けず、同様のサービスダウンを繰り返す可能性が高いと判断した。  
  `pg_logical` やログ出力設定の調整も検討したが、容量を使っていたのは主にバックアップファイルであり、それを残したままでは再発リスクを十分に下げられない。  
  一時的にしのぐだけの対応にとどまるため、継続的な運用方針として採用しないことにした。

#### ADR-02: 打刻データ更新処理異常終了

##### 背景

勤怠打刻データの更新処理で使用するCSVファイル内に、同一社員IDのレコードが複数行含まれていたことが原因で、PHPによる更新処理が異常終了した。  
別システムから出力されたCSVは、本来一意である社員IDが重複する1組のレコードを保持していた。

##### 評価軸

| 評価軸 | 案A：手動で重複レコードを削除し、PHP更新処理を臨時実行 | 案B：PHP更新ロジックを改修し、重複レコードを許容／自動処理 | 案C：客先側でCSVを再出力してもらい、重複を解消してから更新処理を再実行 |
|---|---|---|---|
| 即時復旧性 | ◎ その場でCSVを修正して再実行できるため、当日中の復旧がしやすい | △ 実装・テスト・リリースが必要で、障害対応としては時間がかかる| ✕ 再出力依頼〜再受領までの時間が読めず、実際にも複数回の再出力で解消できなかった |
| データ整合性・監査対応 | 〇 客先と削除対象を合意したうえで記録を残せば整合性は保てる| 〇 ロジック化できれば以降は一貫性ある処理が可能| 〇 正しいCSVが再生成されれば整合性は保たれるが、やり取りの記録管理が必要|
| 作業負荷・属人性| △ CSV内容の理解・判断が必要で、慣れるまでは担当者依存になりやすい| △ 初期コストは高いが、一度実装すれば運用負荷は下がる| △ 調整・連絡の工数が発生し、担当者のコミュニケーション負荷がかかる|
| 再発防止への寄与| △ 手順としては有効だが、根本的には「重複を含むCSVが出てくる」問題は残る| ◎ 重複時の扱い（最新行優先・エラーとして弾く等）を明示でき、再発時も自動処理 | 〇 客先側のCSV生成ロジックが修正されれば、同種のインシデントは減らせる |


##### 採否

- **案A：CSVから重複するレコードを手動削除後、PHP更新処理を臨時実行**  
  → **採用**  
  ※当日の最終対応として採用

- **案B：PHP更新ロジックを改修し、重複レコードを許容／自動処理**  
  → **却下（本インシデント対応としては見送り／後続検討）**

- **案C：客先側でCSVを再出力してもらい、重複を解消してから更新処理を再実行**  
  → **却下（当日は先に試行したが、最終方針としては不採用）**

##### 却下理由

- **案B：PHP更新ロジックを改修し、重複レコードを許容／自動処理**  
  ロジックの改修には設計・実装・テスト・本番反映までの作業が必要であり、当日中に打刻データを確定させるという要件に対して時間的な余裕がなかった。  
  重複時にどのレコードを採用するかなどの業務ルールも、人事・総務側と整理する必要があり、その場で結論を出すことは難しかった。  
  長期的な改善策としては有効と考えたが、本インシデントの初動対応としては適さないと判断し、今回は採用しないことにした。

- **案C：客先側でCSVを再出力してもらい、重複を解消してから更新処理を再実行**  
  当日はまず案Cを選択し、客先に CSV の再出力を複数回依頼したものの、重複や整合性の問題は解消されず更新処理は異常終了を繰り返した。  
  再出力のたびに確認・連絡・再実行が発生し、当日中に処理を完了させるための時間が急速に削られていった。  
  CSV生成側の修正なしに再出力を重ねる運用では締め処理の期限を安定して守れないと判断し、標準的な対応方針としては採用しないことにした。

### 振り返り

#### 技術的な学び

- サーバのディスク構成やリソース状況を踏まえて原因を切り分ける、基本的な運用スキルが身についた。  
- 障害時にも状況を整理しながら段階的に調査・復旧方針を組み立てる感覚を得られた。

#### プロセス・コミュニケーションの学び

- 制約や工数を踏まえて「現実的に取れる選択肢」を選ぶ判断が必要だと実感し、理想とのバランスを意識するようになった。  
- 作業意図や手順を事前に整理して共有することで、確認やレビューが通しやすくなることを学んだ。

#### 現状

- エラー発生時に個人情報を含むログが出力される課題が残っている。  
- 調査のしやすさと情報保護の両立を意識したログ出力方針を検討中である。